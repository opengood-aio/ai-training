{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyN2fBThgo8wJQn6Xf6V6crC"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "### Layout\n",
    "\n",
    "* Images:\n",
    "\t* Dog\n",
    "    * Cat\n",
    "* 1000s of images\n",
    "\t* Training set\n",
    "\t    * 4000 images each for dogs and cats\n",
    "    * Test set\n",
    "        * 1000 images each for dogs and cat\n",
    "\n",
    "### Goals\n",
    "\n",
    "* Build a CNN model to classify images for a dog or cat"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMefrVPCg-60"
   },
   "source": "## Import Libraries"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sCV30xyVhFbE",
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.102732Z",
     "start_time": "2025-09-02T21:47:26.096745Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FIleuCAjoFD8",
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.114672Z",
     "start_time": "2025-09-02T21:47:26.111970Z"
    }
   },
   "source": [
    "tf.__version__"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.20.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxQxCBWyoGPE"
   },
   "source": "## Data Preprocessing"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvE-heJNo3GG"
   },
   "source": "### Preprocessing Training Set"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Image Augmentation\n",
    "\n",
    "* Transformations will be applied to images only in the training set\n",
    "    * This is to avoid over fitting\n",
    "    * Otherwise, there will be a huge difference in accuracy of the training and test sets:\n",
    "        * Close to $98\\%$ accuracy on the training set\n",
    "        * Much lower accuracy on the test set\n",
    "* Geometric transformations are applied to the training set images:\n",
    "    * For example, zoom, rotations, etc. on the images\n",
    "    * First, transvections to shift pixels\n",
    "    * Next, rotations with horizontal flips and zoom-in-and-out\n",
    "* These transformations are called **image augmentation**\n",
    "* The goal is to augment the diversity of the training set images"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Image Generation\n",
    "\n",
    "* The `ImageDataGenerator` class from the `preprocessing.image` module of the Keras library is used to generate images and perform image augmentation\n",
    "    * Parameters\n",
    "        * `rescale` applies feature scaling to each pixel\n",
    "            * Each pixel has a value between $0$ and $255$\n",
    "            * Each pixel value is divided by $255$\n",
    "            * This will normalize the pixel values in images\n",
    "        * `shear_range` randomly applies a range of shear transformations that shift pixels by a specified position to images\n",
    "        * `zoom_range` randomly applies a range of zooming to images\n",
    "        * `horizontal_flip` indicates a value to flip images horizontally\n",
    "* The `train_datagen` variable defines an instance of the `ImageDataGenerator` class\n",
    "* The training dataset will be imported from the dataset directory containing the directory of training set images\n",
    "    * The `flow_from_directory` method on the `ImageDataGenerator` class recursively loads images from a specified directory\n",
    "        * Parameters\n",
    "            * `directory` is the root directory of images to process\n",
    "            * `target_size` performs image resizing by specifying the target pixel size of the images\n",
    "                * This makes the image processing less computationally intensive\n",
    "            * `batch_size` indicates the number of images to process per batch\n",
    "            * `class_mode` specifies the classification mode: `binary` or `categorical`\n",
    "* The `training_set` variable is the training set of the images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0koUcJMJpEBD",
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.128751Z",
     "start_time": "2025-09-02T21:47:26.126975Z"
    }
   },
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.296813Z",
     "start_time": "2025-09-02T21:47:26.139327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "    directory='dataset/training_set',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrCMmGw9pHys"
   },
   "source": "### Preprocessing Test Set"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* Image augmentation is not applied to test set images so those parameters are omitted"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SH4WzfOhpKc3",
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.352927Z",
     "start_time": "2025-09-02T21:47:26.306006Z"
    }
   },
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    directory='dataset/test_set',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af8O4l90gk7B"
   },
   "source": "## Build CNN"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ces1gXY2lmoX"
   },
   "source": "### Initialize CNN"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The `Sequential` class is from the `models` module of the Keras library and allows one to construct a neural network made of a sequence of layers\n",
    "* The `cnn` variable is an object instance of the `Sequential` class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SAUt4UMPlhLS",
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.366294Z",
     "start_time": "2025-09-02T21:47:26.363941Z"
    }
   },
   "source": "cnn = tf.keras.models.Sequential()",
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5YJj_XMl5LF"
   },
   "source": "### Step 1 - Convolution"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The `Conv2D` class is from the `layers` module of the Keras library and creates a convolutional layer\n",
    "    * Parameters\n",
    "        * `filters` is the number of feature detectors used to perform convolution\n",
    "        * `kernel_size` is the size of the array (matrices as row and columns) for a feature detector\n",
    "        * `strides` is the number of pixels to move a feature detector across and up/down an image. Default value of $1$ will be used.\n",
    "        * `activation` is the activation function to apply after convolution\n",
    "        * `input_shape` is the dimensions of an input image in the RGB 3 dimensions of color:\n",
    "            * `width` = $64$ pixels\n",
    "            * `height` = $64$ pixels\n",
    "            * `colors` = $3$ indicates colored images. $0$ = black and $1$ = white."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XPzPrMckl-hV",
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.382960Z",
     "start_time": "2025-09-02T21:47:26.376321Z"
    }
   },
   "source": [
    "cnn.add(\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=(64, 64, 3)\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The `MaxPoolConv2D` class is from the `layers` module of the Keras library and creates a pooling layer using max pooling\n",
    "    * Parameters\n",
    "        * `pool_size` is the size of the array (matrices as row and columns) of the pool window (frame)\n",
    "        * `strides` is the number of pixels to move the pool window across and up/down an image"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ncpqPl69mOac",
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.392963Z",
     "start_time": "2025-09-02T21:47:26.389571Z"
    }
   },
   "source": [
    "cnn.add(\n",
    "    tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2, 2),\n",
    "        strides=(2, 2)\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaTOgD8rm4mU"
   },
   "source": "### Add 2nd Convolutional Layer"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* The `input_shape` parameter is omitted because is the shape is only defined on the first convolutional layer in the CNN"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i_-FZjn_m8gk",
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.402517Z",
     "start_time": "2025-09-02T21:47:26.396547Z"
    }
   },
   "source": [
    "cnn.add(\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.414024Z",
     "start_time": "2025-09-02T21:47:26.410460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn.add(\n",
    "    tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2, 2),\n",
    "        strides=(2, 2)\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The `Flatten` class is from the `layers` module of the Keras library and creates a flattening layer\n",
    "    * No parameters are required for an instance of this class"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.422765Z",
     "start_time": "2025-09-02T21:47:26.418680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn.add(\n",
    "    tf.keras.layers.Flatten()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The `Dense` class is from the `layers` module of the Keras library and allows one to add a fully connected layer to an ANN\n",
    "    * An object instance of this class is used to construct a connected layer\n",
    "    * Parameters\n",
    "        * `units` defines the number of hidden neurons in a hidden layer\n",
    "            * $128$ neurons are used to achieve a higher level of accuracy\n",
    "        * `activation` defines the activation function\n",
    "            * Rectifier activation function (`relu`) will be used for hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8GtmUlLd26Nq",
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.433352Z",
     "start_time": "2025-09-02T21:47:26.426744Z"
    }
   },
   "source": [
    "cnn.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=128,\n",
    "        activation='relu'\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The same `Dense` class is used to construct the output layer except it:\n",
    "    * Has $1$ neuron\n",
    "        * Since there is only $1$ dependent variable\n",
    "    * Has Sigmoid activation function\n",
    "        * Used when making predictions that are binary for classification"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1p_Zj1Mc3Ko_",
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.444045Z",
     "start_time": "2025-09-02T21:47:26.438295Z"
    }
   },
   "source": [
    "cnn.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=1,\n",
    "        activation='sigmoid'\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6XkI90snSDl"
   },
   "source": "## Training CNN"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfrFQACEnc6i"
   },
   "source": "### Compile CNN"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The `compile` method of `Sequential` class compiles the neural network\n",
    "    * Parameters\n",
    "        * `optimizer` defines the algorithm used to minimize loss\n",
    "            * Calculates the gradient of the loss function\n",
    "            * Updates the weights by moving in the direction of the negative gradient\n",
    "            * Optimizer will eventually converge on the global minimum, which is the acceptable level of error\n",
    "            * `adam` is a very performant optimizer that performs stochastic gradient descent (SGD)\n",
    "        * `loss` defines the algorithm to compute the loss function value, which is the difference between predictions and actual values\n",
    "            * When making binary predictions, for classification, use the `binary_crossentropy` loss function\n",
    "            * When making one or two categorical predictions, for classification, use the `categorical_crossentropy` loss function\n",
    "            * When making continuous number predictions, for regression, use the `mean_squared_error` loss function\n",
    "        * `metrics` defines the list of metrics to be evaluated by the model during training and testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NALksrNQpUlJ",
    "ExecuteTime": {
     "end_time": "2025-09-02T21:47:26.452328Z",
     "start_time": "2025-09-02T21:47:26.448371Z"
    }
   },
   "source": [
    "cnn.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehS-v3MIpX2h"
   },
   "source": "### Train CNN on Training Set and Evaluating on Test Set"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The `fit` method of `Sequential` class trains the neural network on the training set and validates it using the test set\n",
    "    * Parameters\n",
    "        * `x` defines the matrix of features for the training set\n",
    "        * `validation_data` defines the dataset used for validation of the model\n",
    "            * The validation dataset is used to monitor the training and make adjustments to the neural network parameters if needed\n",
    "            * This is a way to prevent overfitting\n",
    "            * The validation dataset is not used to make predictions\n",
    "        * `epochs` defines the number of full iterations of a dataset\n",
    "            * A neural network must be trained over $n$ number of epochs to improve its accuracy over time"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XUj1W4PJptta",
    "ExecuteTime": {
     "end_time": "2025-09-02T21:51:18.309155Z",
     "start_time": "2025-09-02T21:47:26.458542Z"
    }
   },
   "source": [
    "cnn.fit(\n",
    "    x=training_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=25\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 36ms/step - accuracy: 0.5444 - loss: 0.6932 - val_accuracy: 0.6275 - val_loss: 0.6466\n",
      "Epoch 2/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 37ms/step - accuracy: 0.6548 - loss: 0.6297 - val_accuracy: 0.6960 - val_loss: 0.5921\n",
      "Epoch 3/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 37ms/step - accuracy: 0.6801 - loss: 0.5909 - val_accuracy: 0.7200 - val_loss: 0.5658\n",
      "Epoch 4/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 37ms/step - accuracy: 0.7165 - loss: 0.5567 - val_accuracy: 0.7180 - val_loss: 0.5536\n",
      "Epoch 5/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 37ms/step - accuracy: 0.7339 - loss: 0.5263 - val_accuracy: 0.7495 - val_loss: 0.5226\n",
      "Epoch 6/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.7641 - loss: 0.4934 - val_accuracy: 0.7620 - val_loss: 0.5006\n",
      "Epoch 7/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 37ms/step - accuracy: 0.7727 - loss: 0.4723 - val_accuracy: 0.7665 - val_loss: 0.4811\n",
      "Epoch 8/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 39ms/step - accuracy: 0.7764 - loss: 0.4621 - val_accuracy: 0.7690 - val_loss: 0.4836\n",
      "Epoch 9/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 39ms/step - accuracy: 0.7733 - loss: 0.4589 - val_accuracy: 0.7625 - val_loss: 0.4970\n",
      "Epoch 10/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 37ms/step - accuracy: 0.7948 - loss: 0.4419 - val_accuracy: 0.7735 - val_loss: 0.4981\n",
      "Epoch 11/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 37ms/step - accuracy: 0.8048 - loss: 0.4235 - val_accuracy: 0.7500 - val_loss: 0.5207\n",
      "Epoch 12/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 38ms/step - accuracy: 0.8060 - loss: 0.4200 - val_accuracy: 0.7790 - val_loss: 0.4737\n",
      "Epoch 13/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 38ms/step - accuracy: 0.8071 - loss: 0.4141 - val_accuracy: 0.7815 - val_loss: 0.4702\n",
      "Epoch 14/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 38ms/step - accuracy: 0.8242 - loss: 0.3834 - val_accuracy: 0.7780 - val_loss: 0.4949\n",
      "Epoch 15/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 38ms/step - accuracy: 0.8168 - loss: 0.3940 - val_accuracy: 0.7930 - val_loss: 0.4561\n",
      "Epoch 16/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.8277 - loss: 0.3825 - val_accuracy: 0.7810 - val_loss: 0.5003\n",
      "Epoch 17/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.8335 - loss: 0.3547 - val_accuracy: 0.7930 - val_loss: 0.4830\n",
      "Epoch 18/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.8568 - loss: 0.3321 - val_accuracy: 0.7920 - val_loss: 0.4783\n",
      "Epoch 19/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.8477 - loss: 0.3391 - val_accuracy: 0.8010 - val_loss: 0.4809\n",
      "Epoch 20/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.8471 - loss: 0.3395 - val_accuracy: 0.8005 - val_loss: 0.4910\n",
      "Epoch 21/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 37ms/step - accuracy: 0.8608 - loss: 0.3166 - val_accuracy: 0.7585 - val_loss: 0.5988\n",
      "Epoch 22/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.8634 - loss: 0.3071 - val_accuracy: 0.8040 - val_loss: 0.4972\n",
      "Epoch 23/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.8833 - loss: 0.2808 - val_accuracy: 0.8035 - val_loss: 0.4903\n",
      "Epoch 24/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.8812 - loss: 0.2820 - val_accuracy: 0.7895 - val_loss: 0.5128\n",
      "Epoch 25/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.8882 - loss: 0.2558 - val_accuracy: 0.7845 - val_loss: 0.5429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x11f755f70>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3PZasO0006Z"
   },
   "source": "## Make Predictions"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Image Preprocessing Module"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T21:52:41.121131Z",
     "start_time": "2025-09-02T21:52:41.118905Z"
    }
   },
   "cell_type": "code",
   "source": "from keras.preprocessing import image",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Make Single Prediction"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The `load_img` function of the `image` module of the Keras library loads an image from a specified file path\n",
    "    * Parameters\n",
    "        * `path` is the file path to the image to load\n",
    "        * `target_size` performs image resizing by specifying the target pixel size of the image"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load Image"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gsSiWEJY1BPB",
    "ExecuteTime": {
     "end_time": "2025-09-02T22:14:38.001321Z",
     "start_time": "2025-09-02T22:14:37.994671Z"
    }
   },
   "source": [
    "test_image = image.load_img(\n",
    "    path='dataset/single_prediction/cat_or_dog_1.png',\n",
    "    target_size=(64, 64)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Convert PIL Image Format to Image Array"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* A \"PIL image\" refers to an image object created and manipulated using the Python Imaging Library (PIL), or more commonly, its actively maintained fork, Pillow\n",
    "* The `img_to_array` function of the `image` module of the Keras library converts an image to a Numpy array\n",
    "    * Parameters\n",
    "        * `img` is the image object to convert\n",
    "    * The function returns a 2D Numpy array"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:14:40.397312Z",
     "start_time": "2025-09-02T22:14:40.395461Z"
    }
   },
   "cell_type": "code",
   "source": "test_image = image.img_to_array(img=test_image)",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Add Batch Dimension to Image Array"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The CNN model was trained using batches of images\n",
    "* To make a single prediction, the image must be added to a batch of images\n",
    "    * This is done by adding a batch dimension to the image array\n",
    "* The `expand_dims` function of the Numpy library adds a batch dimension to a Numpy array\n",
    "    * Parameters\n",
    "        * `a` is the image array in which to add a batch dimension\n",
    "        * `axis` is the index in which add the batch dimension\n",
    "            * It is the first dimension since batches are first in the ordering of objects used to train the CNN model"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:14:42.429057Z",
     "start_time": "2025-09-02T22:14:42.427179Z"
    }
   },
   "cell_type": "code",
   "source": "test_image = np.expand_dims(a=test_image, axis=0)",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Make Prediction"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:16:10.647397Z",
     "start_time": "2025-09-02T22:16:10.590680Z"
    }
   },
   "cell_type": "code",
   "source": "result = cnn.predict(test_image)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 33ms/step\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Display Classes"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:17:27.206372Z",
     "start_time": "2025-09-02T22:17:27.204032Z"
    }
   },
   "cell_type": "code",
   "source": "training_set.class_indices",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Display Result"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The `result` object contains\n",
    "    * The batch dimension at index $0$\n",
    "    * Then the predicted value at index $0$"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:29:06.310242Z",
     "start_time": "2025-09-02T22:29:06.307937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if result[0][0] == 1:\n",
    "    prediction='Dog'\n",
    "else:\n",
    "    prediction='Cat'"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:29:07.348601Z",
     "start_time": "2025-09-02T22:29:07.345871Z"
    }
   },
   "cell_type": "code",
   "source": "print(prediction)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    }
   ],
   "execution_count": 49
  }
 ]
}
