{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "natural_language_processing.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyMx/KsxUDrn2M5QbIb03B9p"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwK5-9FIB-lu",
    "colab_type": "text"
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "### Layout\n",
    "\n",
    "* Columns:\n",
    "\t* Review\n",
    "\t    * Contains the text each customer submitted for a review\n",
    "    * Liked\n",
    "* Rows: 100s of observations\n",
    "\t* Each row represents a review submitted by a customer indicating if the customer liked a restaurant or not\n",
    "\t    * 1 = customer liked a restaurant\n",
    "        * 0 = customer did not like a restaurant\n",
    "* Dataset file is a tab-separated values (TSV) file instead of CSV since review texts can contain commas\n",
    "\n",
    "### Background\n",
    "\n",
    "* One is a data scientist working for a group of restaurants\n",
    "* Owners of the restaurants group want to analyze restaurant reviews for customer sentiment from their submitted reviews to determine if a customer liked a restaurant or not\n",
    "\n",
    "### Goals\n",
    "\n",
    "* Build a Bag of Words model to pre-process the text from restaurant reviews\n",
    "* Build a classification model to determine if a customer will like a restaurant or not based on sentiment from reviews\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:13:42.428529Z",
     "start_time": "2025-06-12T23:13:42.421281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Dataset"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The review column of the TSV file contains free-form text containing customer restaurant reviews\n",
    "* The review texts sometimes contain double quotes\n",
    "* When cleaning the texts, one must indicate to the model to ignore double quotes, which can othwerwise lead to processing errors\n",
    "* Set the `quoting` parameter to $3$ in the Pandas `read_csv` function to ignore reading double quotes into the dataset"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:13:42.444190Z",
     "start_time": "2025-06-12T23:13:42.438682Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter='\\t', quoting=3)",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qekztq71CixT",
    "colab_type": "text"
   },
   "source": "## Clean Texts"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Additional Libraries"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* `re` library provides support for regular expressions\n",
    "* `nltk` is the Natural Language Toolkit library for working with human language data\n",
    "    * It provides tools for various tasks in NLP:\n",
    "        * **Tokenization:** Splitting text into words or sentences\n",
    "        * **Stop word removal:** Filtering out common words such as *the*, *is*, *an*, etc.\n",
    "        * **Stemming:** Reducing words to their root form\n",
    "        * **Part-of-speech tagging:** Labeling words with their grammatical roles\n",
    "        * **Lemmatization:** Reducing words to their base form\n",
    "        * **Parsing:** Analyzing the grammatical structure of sentences"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:13:42.456055Z",
     "start_time": "2025-06-12T23:13:42.454379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import nltk"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download Stop Words"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:13:42.468768Z",
     "start_time": "2025-06-12T23:13:42.466166Z"
    }
   },
   "cell_type": "code",
   "source": "nltk.download('stopwords')",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cjaehnen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Downloaded Stop Words"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:13:42.503132Z",
     "start_time": "2025-06-12T23:13:42.499301Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.corpus import stopwords",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Stemming Class"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* `PorterStemmer` class reduces a word to its root form, indicating enough about what a word means\n",
    "    * For example, a review contains the word *loved*\n",
    "        * Stemming will transform *loved* into *love*\n",
    "        * Simplifies the word to its root form\n",
    "    * Goal is to remove all conjugations and keep the present tense of a word\n",
    "* When the Bag of Words model is created:\n",
    "    * One will create a sparse matrix where each column will have all the different words from all reviews\n",
    "    * One wants to minimize the dimension of the sparse matrix\n",
    "    * The dimension is the number of columns\n",
    "    * Applying stemming minimizes the number of words in the sparse matrix\n",
    "    * Without stemming applied, different conjugations of words would be included in the sparse matrix\n",
    "    * Stemming will reduce the dimension of the sparse matrix"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:13:42.520683Z",
     "start_time": "2025-06-12T23:13:42.518115Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.stem.porter import PorterStemmer",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initialize Variables"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* `corpus` is a list of all cleaned texts from all reviews\n",
    "* `stop_words` is a set of English language stop words"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:13:42.538381Z",
     "start_time": "2025-06-12T23:13:42.535270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = []\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Loop Iterating Over Reviews"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* `i` is the iterator variable for each review\n",
    "* The upper bound of the range is the size of rows in the dataset\n",
    "* `review` is the review with cleaned text"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Text Cleaning"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 1: Remove All Punctuations\n",
    "\n",
    "* `re.sub` function call replaces all non-alphanumeric characters with a space\n",
    "    * The first parameter takes a regular expression using the not operator `^` matching text where it is not alphanumeric characters\n",
    "    * The second parameter takes the replacement regular expression of a space\n",
    "    * The third parameter is the review text from the review column in the dataset\n",
    "\n",
    "#### Step 2: Normalization\n",
    "\n",
    "* Transform all uppercase alphanumeric characters to lowercase\n",
    "* `lower` function call converts all uppercase alphanumeric characters to lowercase\n",
    "\n",
    "#### Step 3: Splitting\n",
    "\n",
    "* Split review texts into words\n",
    "* `split` function call splits review texts into a list of words\n",
    "\n",
    "#### Step 4: Stemming and Stop Word Removal\n",
    "\n",
    "* `ps` is the object instance of the `PorterStemmer` class used to apply stemming\n",
    "* For loop iterates through all the words in the new `review` list\n",
    "* `word` is the iterator variable for each review word\n",
    "* To remove stop words, Python allows for an inline conditional check, in the for loop, to omit values from the iterator when the condition is not true\n",
    "    * `word` is checked against all the English language stop words\n",
    "    * If found, `word` is omitted\n",
    "    * Otherwise, `word` is included\n",
    "* `ps` object method `stem` call applies stemming to a word\n",
    "* After all words in `review` list have stop words removed and are stemmed, one needs to join all the words back into a string\n",
    "    * To join all the words in the list, separated by a space, the `join` method is called the on space `' '` string object and takes the list as a parameter\n",
    "\n",
    "#### Step 5: Add Cleansed Text to Cleansed List\n",
    "\n",
    "* Final step is to append the cleansed `review` text to the `corpus` list"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:28:26.933604Z",
     "start_time": "2025-06-12T23:28:26.884209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(0, len(dataset)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in stop_words]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Bag of Words Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:13:42.576394Z",
     "start_time": "2025-06-12T23:13:42.574766Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split Dataset into Training Set and Test Set"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T23:13:42.722747Z",
     "start_time": "2025-06-12T23:13:42.590078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodel_selection\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m X_train, X_test, y_train, y_test = train_test_split(\u001B[43mX\u001B[49m, y, test_size=\u001B[32m0.2\u001B[39m, random_state=\u001B[32m0\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'X' is not defined"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Naive Bayes Model on Training Set"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predict Test Set Results"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_pred = classifier.predict(X_test)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1))"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Making the Confusion Matrix"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(cm)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Results:\n",
    "\n",
    "* $0$ true negatives\n",
    "* $0$ false positives\n",
    "* $0$ false negatives\n",
    "* $0$ true positives"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compute Accuracy Score"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Interpreting Results\n",
    "\n",
    "*\n",
    "\n",
    "## Takeaways\n",
    "\n",
    "*"
   ]
  }
 ]
}
