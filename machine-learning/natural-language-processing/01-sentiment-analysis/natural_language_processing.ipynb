{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "natural_language_processing.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyMx/KsxUDrn2M5QbIb03B9p"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwK5-9FIB-lu",
    "colab_type": "text"
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "### Layout\n",
    "\n",
    "* Columns:\n",
    "\t* Review\n",
    "\t    * Contains the text each customer submitted for a review\n",
    "    * Liked\n",
    "* Rows: 100s of observations\n",
    "\t* Each row represents a review submitted by a customer indicating if the customer liked a restaurant or not\n",
    "\t    * 1 = customer liked a restaurant\n",
    "        * 0 = customer did not like a restaurant\n",
    "* Dataset file is a tab-separated values (TSV) file instead of CSV since review texts can contain commas\n",
    "\n",
    "### Background\n",
    "\n",
    "* One is a data scientist working for a group of restaurants\n",
    "* Owners of the restaurants group want to analyze restaurant reviews for customer sentiment from their submitted reviews to determine if a customer liked a restaurant or not\n",
    "\n",
    "### Goals\n",
    "\n",
    "* Build a Bag of Words model to pre-process the text from restaurant reviews\n",
    "* Build a classification model to determine if a customer will like a restaurant or not based on sentiment from reviews\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T23:39:07.184429Z",
     "start_time": "2025-06-10T23:39:05.376800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Dataset"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The review column of the TSV file contains free-form text containing customer restaurant reviews\n",
    "* The review texts sometimes contain double quotes\n",
    "* When cleaning the texts, one must indicate to the model to ignore double quotes, which can othwerwise lead to processing errors\n",
    "* Set the `quoting` parameter to $3$ in the Pandas `read_csv` function to ignore reading double quotes into the dataset"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T23:39:07.972509Z",
     "start_time": "2025-06-10T23:39:07.967719Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter='\\t', quoting=3)",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qekztq71CixT",
    "colab_type": "text"
   },
   "source": "## Clean Texts"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Additional Libraries"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* `re` library provides support for regular expressions\n",
    "* `nltk` is the Natural Language Toolkit library for working with human language data\n",
    "    * It provides tools for various tasks in NLP:\n",
    "        * **Tokenization:** Splitting text into words or sentences\n",
    "        * **Stop word removal:** Filtering out common words such as *the*, *is*, *an*, etc.\n",
    "        * **Stemming:** Reducing words to their root form\n",
    "        * **Part-of-speech tagging:** Labeling words with their grammatical roles\n",
    "        * **Lemmatization:** Reducing words to their base form\n",
    "        * **Parsing:** Analyzing the grammatical structure of sentences"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T23:39:17.743Z",
     "start_time": "2025-06-10T23:39:12.560076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import nltk"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download Stop Words"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T23:39:19.700824Z",
     "start_time": "2025-06-10T23:39:19.352565Z"
    }
   },
   "cell_type": "code",
   "source": "nltk.download('stopwords')",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cjaehnen/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Downloaded Stop Words"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T23:39:25.907205Z",
     "start_time": "2025-06-10T23:39:25.905097Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.corpus import stopwords",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Stemming Class"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* `PorterStemmer` class reduces a word to its root form, indicating enough about what a word means\n",
    "    * For example, a review contains the word *loved*\n",
    "        * Stemming will transform *loved* into *love*\n",
    "        * Simplifies the word to its root form\n",
    "    * Goal is to remove all conjugations and keep the present tense of a word\n",
    "* When the Bag of Words model is created:\n",
    "    * One will create a sparse matrix where each column will have all the different words from all reviews\n",
    "    * One wants to minimize the dimension of the sparse matrix\n",
    "    * The dimension is the number of columns\n",
    "    * Applying stemming minimizes the number of words in the sparse matrix\n",
    "    * Without stemming applied, different conjugations of words would be included in the sparse matrix\n",
    "    * Stemming will reduce the dimension of the sparse matrix"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T23:54:17.680974Z",
     "start_time": "2025-06-10T23:54:17.678974Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.stem.porter import PorterStemmer",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initialize Variables"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* `corpus` is a list of all cleaned texts from all reviews"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T00:05:40.196226Z",
     "start_time": "2025-06-11T00:05:40.194065Z"
    }
   },
   "cell_type": "code",
   "source": "corpus = []",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Loop Iterating Over Reviews"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* `i` is the iterator variable for each review\n",
    "* The upper bound of the range is the size of rows in the dataset\n",
    "* `review` is the review with cleaned text"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Text Cleaning"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 1: Remove All Punctuations\n",
    "\n",
    "* `re.sub` function call replaces all non-alphanumeric characters with a space\n",
    "    * The first parameter takes a regular expression using the not operator `^` matching text where it is not alphanumeric characters\n",
    "    * The second parameter takes the replacement regular expression of a space\n",
    "    * The third parameter is the review text from the review column in the dataset\n",
    "\n",
    "#### Step 2: Normalization\n",
    "\n",
    "* Transform all uppercase alphanumeric characters to lowercase\n",
    "* `lower` function call converts all uppercase alphanumeric characters to lowercase\n",
    "\n",
    "#### Step 3: Tokenization\n",
    "\n",
    "* Split review texts into words\n",
    "* `split` function call splits review texts into a list of words"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T00:27:34.605529Z",
     "start_time": "2025-06-11T00:27:34.596736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(0, len(dataset)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Bag of Words Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split Dataset into Training Set and Test Set"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Naive Bayes Model on Training Set"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predict Test Set Results"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_pred = classifier.predict(X_test)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1))"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Making the Confusion Matrix"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(cm)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Results:\n",
    "\n",
    "* $0$ true negatives\n",
    "* $0$ false positives\n",
    "* $0$ false negatives\n",
    "* $0$ true positives"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compute Accuracy Score"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Interpreting Results\n",
    "\n",
    "*\n",
    "\n",
    "## Takeaways\n",
    "\n",
    "*"
   ]
  }
 ]
}
