{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MRC0e0KhQ0S"
   },
   "source": "# Tokenizers"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Outlines creating and using models using the `AutoTokenizer` class from the Hugging Face `Transformers` library\n",
    "* All classes and functions are imported from the `Transformers` library"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T20:37:45.548052Z",
     "start_time": "2025-11-02T20:37:45.545650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_provider = \"bert\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = f\"{model_provider}/{model_name}\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## What is a Tokenizer?"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Tokenizers serve the purpose to translate text into data that can be processed by a model\n",
    "* Models can only process numbers\n",
    "* Tokenizers need to convert text inputs into numerical data\n",
    "* In NLP tasks, data is that is generally processed is raw text: `Jim Henson was a puppeteer`\n",
    "* The goal is to find the most meaningful representation of text converted to numbers as input to a model\n",
    "* Where possible, find the smallest representation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Types of Tokenizers"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Word-based"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Word-based tokenizers split raw text into words and assign a numerical representation for each of them\n",
    "* There are different methods to split text into words\n",
    "* For example, using Python's split function on whitespace:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T20:37:45.560997Z",
     "start_time": "2025-11-02T20:37:45.559258Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_text = \"Jim Henson was a puppeteer\".split()",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T20:37:45.577892Z",
     "start_time": "2025-11-02T20:37:45.575868Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenized_text)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* There are also variations of word tokenizers that have extra rules for punctuation\n",
    "* With this kind of tokenizer, one can end up with some pretty large \"vocabularies\"\n",
    "* **A vocabulary is defined by the total number of independent tokens that one has in the corpus (collection of texts)**\n",
    "* Each word gets assigned an ID\n",
    "\t* Starting from 0\n",
    "\t* And going up to the size of the vocabulary\n",
    "* The model uses these IDs to identify each word\n",
    "* Words like \"dog\" are represented differently from words like \"dogs\"\n",
    "\t* The model will initially have no way of knowing that \"dog\" and \"dogs\" are similar\n",
    "\t* It will identify the two words as unrelated\n",
    "* The same applies to other similar words, like \"run\" and \"running\"\n",
    "\t* The model will not see these as being similar initially\n",
    "* Finally, one needs a custom token to represent words that are not in the vocabulary\n",
    "* This is known as the \"unknown\" token, often represented as:\n",
    "\t* \\[UNK\\]\n",
    "\t* \\<unk\\>\n",
    "* It is generally a bad sign if one sees that the tokenizer is producing a lot of these tokens\n",
    "\t* As it was not able to retrieve a sensible representation of a word\n",
    "\t* And one is losing information along the way\n",
    "* The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token\n",
    "* One way to reduce the amount of unknown tokens is to go one level deeper, using a character-based tokenizer"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Character-based"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Character-based tokenizers split the text into characters, rather than words.\n",
    "* This has two primary benefits:\n",
    "    * The vocabulary is much smaller\n",
    "    * There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters\n",
    "* This approach is not perfect either\n",
    "* Since the representation is now based on characters rather than words, one could argue that, intuitively, it is less meaningful:\n",
    "\t* Each character does not mean a lot on its own\n",
    "\t* Whereas that is the case with words\n",
    "* Another thing to consider is that one will end up with a huge number of tokens to be processed by the model:\n",
    "\t* Whereas a word would only be a single token with a word-based tokenizer\n",
    "\t* It can easily turn into 10 or more tokens when converted into characters\n",
    "* To get the best of both worlds, one can use a third technique that combines the two approaches: *subword tokenization*"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Subword"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords\n",
    "* For instance, \"annoyingly\" might be considered a rare word and could be decomposed into \"annoying\" and \"ly\"\n",
    "* These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of \"annoyingly\" is kept by the composite meaning of \"annoying\" and \"ly\"\n",
    "* These subwords end up providing a lot of semantic meaning:\n",
    "\t* For instance, in the example above \"tokenization\" was split into \"token\" and \"ization\"\n",
    "\t* Two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word)\n",
    "* This allows one to have relatively good coverage with small vocabularies, and close to no unknown tokens"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading and Saving"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Loading and saving tokenizers is as simple as it is with models\n",
    "* It is based on the same two methods:\n",
    "\t* `from_pretrained`\n",
    "\t* `save_pretrained`\n",
    "* These methods will load or save the algorithm used by the tokenizer (a bit like the architecture of the model) as well as its vocabulary (a bit like the weights of the model)\n",
    "* Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except one uses the `AutoTokenizer` class:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T20:45:07.421407Z",
     "start_time": "2025-11-02T20:45:07.232740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T20:47:10.918977Z",
     "start_time": "2025-11-02T20:47:10.907517Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer(\"Using a Transformer network is simple\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Encoding"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* **Translating text to numbers is known as encoding**\n",
    "* Encoding is done in a two-step process:\n",
    "\t* The tokenization\n",
    "\t* Followed by the conversion to input IDs\n",
    "* The first step is to split the text into words (or parts of words, punctuation symbols, etc.), called tokens\n",
    "* There are multiple rules that can govern that process, which is why one needs to instantiate the tokenizer using the name of the model, to make sure one uses the same rules that were used when the model was pre-trained\n",
    "* The second step is to convert those tokens into numbers, so one can build a tensor out of them and feed them to the model\n",
    "* To do this, the tokenizer has a vocabulary, which is the part one downloads when one instantiates it with the `from_pretrained` method"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenization"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The tokenization process is done by the `tokenize` method of the tokenizer\n",
    "* The output of this method is a list of strings or tokens\n",
    "* This tokenizer is a subword tokenizer: it splits the words until it gets tokens that can be represented by its vocabulary\n",
    "* That is the case here with transformer, which is split into two tokens:\n",
    "\t* transform\n",
    "\t* ##er"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T20:50:24.118440Z",
     "start_time": "2025-11-02T20:50:23.732368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T20:50:34.805915Z",
     "start_time": "2025-11-02T20:50:34.803080Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokens)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## From Tokens to Input IDs"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The conversion to input IDs is handled by the `convert_tokens_to_ids` tokenizer method\n",
    "* These outputs, once converted to the appropriate framework tensor, can then be used as inputs to a model"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T20:51:40.472810Z",
     "start_time": "2025-11-02T20:51:40.470403Z"
    }
   },
   "cell_type": "code",
   "source": "ids = tokenizer.convert_tokens_to_ids(tokens)",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T20:51:41.669189Z",
     "start_time": "2025-11-02T20:51:41.666183Z"
    }
   },
   "cell_type": "code",
   "source": "print(ids)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decoding"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Decoding is going the other way around: from vocabulary indices, one wants to get a string\n",
    "* This can be done with the `decode` method\n",
    "* Note that the `decode` method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence\n",
    "* This behavior will be extremely useful when one uses models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T20:52:45.446638Z",
     "start_time": "2025-11-02T20:52:45.442544Z"
    }
   },
   "cell_type": "code",
   "source": "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T20:52:46.532710Z",
     "start_time": "2025-11-02T20:52:46.530018Z"
    }
   },
   "cell_type": "code",
   "source": "print(decoded_string)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ]
}
