{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MRC0e0KhQ0S"
   },
   "source": "# Putting It All Together"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Outlines putting together all the usages from tokenizer to model using the `AutoTokenizer` and `AutoModel` classes from the Hugging Face `Transformers` library\n",
    "* All classes and functions are imported from the `Transformers` library"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T22:49:13.524608Z",
     "start_time": "2025-11-02T22:49:13.520175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_provider = \"distilbert\"\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = f\"{model_provider}/{model_name}\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Mimicking the Pipeline Function"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The Transformers API can handle all of this with a high-level function that one will dive into here\n",
    "* When one calls one's tokenizer directly on the sentence, one gets back inputs that are ready to pass through one's model:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T22:49:13.742354Z",
     "start_time": "2025-11-02T22:49:13.540344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* Here, the `model_inputs` variable contains everything that is necessary for a model to operate well"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* As one will see in some examples below, this method is very powerful\n",
    "* First, it can tokenize a single sequence:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T22:49:13.750894Z",
     "start_time": "2025-11-02T22:49:13.748907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It also handles multiple sequences at a time, with no change in the API:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T22:49:13.757224Z",
     "start_time": "2025-11-02T22:49:13.755390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It can pad according to several objectives:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T22:49:13.763600Z",
     "start_time": "2025-11-02T22:49:13.760953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# will pad the sequences up to the model max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It can also truncate sequences:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T22:49:13.785025Z",
     "start_time": "2025-11-02T22:49:13.782526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# will truncate the sequences that are longer than the model max length\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The `tokenizer` object can handle the conversion to specific framework tensors, which can then be directly sent to the model\n",
    "* For example, in the following code sample, one is prompting the tokenizer to return tensors from the different frameworks:\n",
    "\t* `\"pt\"` returns PyTorch tensors\n",
    "\t* `\"np\"` returns NumPy arrays"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T22:49:13.809448Z",
     "start_time": "2025-11-02T22:49:13.799868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Special Tokens"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If one takes a look at the input IDs returned by the tokenizer, one will see they are a tiny bit different from what one had earlier:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T22:49:13.823834Z",
     "start_time": "2025-11-02T22:49:13.821657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T22:49:13.842974Z",
     "start_time": "2025-11-02T22:49:13.841014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model_inputs[\"input_ids\"])\n",
    "print(ids)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* One token ID was added at the beginning, and one at the end\n",
    "* Let us decode the two sequences of IDs above to see what this is about:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T22:49:13.868369Z",
     "start_time": "2025-11-02T22:49:13.866190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end\n",
    "* This is because the model was pre-trained with those, so to get the same results for inference one needs to add them as well\n",
    "* Note that some models do not add special words, or add different ones\n",
    "* Models may also add these special words only at the beginning, or only at the end\n",
    "* In any case, the tokenizer knows which ones are expected and will deal with this for one"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## From Tokenizer to Model"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Now that one has seen all the individual steps the `tokenizer` object uses when applied on texts\n",
    "* Let us see one final time how it can handle:\n",
    "\t* Multiple sequences\n",
    "\t* Padding\n",
    "\t* Very long sequences\n",
    "\t* Truncation\n",
    "\t* Multiple types of tensors\n",
    "\n",
    "\twith its main API:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T22:49:14.559192Z",
     "start_time": "2025-11-02T22:49:13.879227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)"
   ],
   "outputs": [],
   "execution_count": 14
  }
 ]
}
