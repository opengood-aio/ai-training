{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MRC0e0KhQ0S"
   },
   "source": "# Optimized Inference Deployment"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* Instructions assumes one is using an M-series processor on macOS"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TGI"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* HuggingFace does not currently support `arm64` platform architectures, such as the M-series processors for Macs\n",
    "* Docker does not support access to the native macOS Metal GPUs\n",
    "* Currently, the TGI image cannot be run in Docker on macOS\n",
    "* Instructions documented below show how to:\n",
    "    * Run the TGI image\n",
    "    * Use the `InferenceClient` to generate text from the TGI endpoint\n",
    "    * Use for chat format"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "docker run --gpus all \\\n",
    "    --platform linux/amd64 \\\n",
    "    --shm-size 1g \\\n",
    "    -p 8080:80 \\\n",
    "    -v ~/.cache/huggingface:/data \\\n",
    "    ghcr.io/huggingface/text-generation-inference:latest \\\n",
    "    --model-id HuggingFaceTB/SmolLM2-360M-Instruct\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use HuggingFace `InferenceClient` to Access TGI Endpoint"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# initialize client pointing to TGI endpoint\n",
    "client = InferenceClient(\n",
    "    model=\"http://localhost:8080\",  # URL to the TGI server\n",
    ")\n",
    "\n",
    "# text generation\n",
    "response = client.text_generation(\n",
    "    \"Tell me a story\",\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    details=True,\n",
    "    stop_sequences=[],\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.generated_text)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use for Chat Format"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# chat completion\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.choices[0].message.content)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use OpenAI Client"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# initialize client pointing to TGI endpoint\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",  # Make sure to include /v1\n",
    "    api_key=\"not-needed\",  # TGI doesn't require an API key by default\n",
    ")\n",
    "\n",
    "# chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.choices[0].message.content)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Llama.cpp"
  }
 ]
}
