{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MRC0e0KhQ0S"
   },
   "source": "# Optimized Inference Deployment"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* Instructions assumes one is using an M-series processor on macOS"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TGI"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* HuggingFace does not currently support `arm64` platform architectures, such as the M-series processors for Macs\n",
    "* Docker does not support access to the native macOS Metal GPUs\n",
    "* Currently, the TGI image cannot be run in Docker on macOS\n",
    "* Instructions documented below show how to:\n",
    "    * Run the TGI image\n",
    "    * Use the `InferenceClient` to generate text from the TGI endpoint\n",
    "    * Use for chat format"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run Docker Image"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "docker run --gpus all \\\n",
    "    --platform linux/amd64 \\\n",
    "    --shm-size 1g \\\n",
    "    -p 8080:80 \\\n",
    "    -v ~/.cache/huggingface:/data \\\n",
    "    ghcr.io/huggingface/text-generation-inference:latest \\\n",
    "    --model-id HuggingFaceTB/SmolLM2-360M-Instruct\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use HuggingFace `InferenceClient` to Access TGI Server"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# initialize client pointing to TGI endpoint\n",
    "client = InferenceClient(\n",
    "    model=\"http://localhost:8080\",  # URL to the TGI server\n",
    ")\n",
    "\n",
    "# text generation\n",
    "response = client.text_generation(\n",
    "    \"Tell me a story\",\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    details=True,\n",
    "    stop_sequences=[],\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.generated_text)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use for Chat Format"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# chat completion\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.choices[0].message.content)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use OpenAI Client"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# initialize client pointing to TGI endpoint\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",  # Make sure to include /v1\n",
    "    api_key=\"not-needed\",  # TGI doesn't require an API key by default\n",
    ")\n",
    "\n",
    "# chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.choices[0].message.content)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Llama.cpp"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "# install via Homebrew\n",
    "brew install llama.cpp\n",
    "\n",
    "# download model and run model directly\n",
    "llama-cli -hf HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF\n",
    "\n",
    "# launch OpenAI-compatible API server\n",
    "llama-server -hf HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use HuggingFace `InferenceClient` to Access Llama.cpp Server"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# initialize client pointing to llama.cpp server\n",
    "client = InferenceClient(\n",
    "    model=\"http://localhost:8080/v1\",  # URL to the llama.cpp server\n",
    "    token=\"sk-no-key-required\",  # llama.cpp server requires this placeholder\n",
    ")\n",
    "\n",
    "# text generation\n",
    "response = client.text_generation(\n",
    "    \"Tell me a story\",\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    details=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(response.generated_text)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use for Chat Format"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:23:47.523966Z",
     "start_time": "2025-11-17T23:23:46.817828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# chat completion\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:23:49.979852Z",
     "start_time": "2025-11-17T23:23:49.976555Z"
    }
   },
   "cell_type": "code",
   "source": "print(response.choices[0].message.content)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a far-off land, there was a young girl named Lily. She lived in a small village surrounded by a magical forest filled with mythical creatures and enchanted trees. Lily was a curious and adventurous young girl, always eager to explore and learn about the world around her.\n",
      "\n",
      "One day, Lily decided to venture into the forest to explore the magical creatures that lived there. She packed a small bag with food and water, and set off on her journey. As she walked deeper into the\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use OpenAI Client"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:24:49.989695Z",
     "start_time": "2025-11-17T23:24:48.781607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# initialize client pointing to llama.cpp server\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",\n",
    "    api_key=\"sk-no-key-required\",  # llama.cpp server requires this placeholder\n",
    ")\n",
    "\n",
    "# chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"smollm2-1.7b-instruct\",  # Model identifier can be anything as server only loads one model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:24:52.612819Z",
     "start_time": "2025-11-17T23:24:52.609868Z"
    }
   },
   "cell_type": "code",
   "source": "print(response.choices[0].message.content)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a young girl named Lily who lived in a small village nestled in the heart of a dense forest. She was known for her kindness and her love for the forest, where she would often explore and learn about the different plants, animals, and insects that lived there. One day, she stumbled upon a small, mysterious-looking door hidden deep within the forest. The door was old and worn, with strange symbols etched into its surface, and it seemed to be made\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## vLLM"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "# launch vLLM OpenAI-compatible server via native python interface\n",
    "VLLM_USE_CUDA=0 python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model HuggingFaceTB/SmolLM2-360M-Instruct \\\n",
    "    --tensor-parallel-size 1 \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --dtype float16\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use HuggingFace `InferenceClient` to Access vLLM Server"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# initialize client pointing to vLLM endpoint\n",
    "client = InferenceClient(\n",
    "    model=\"http://localhost:8000/v1\",  # URL to the vLLM server\n",
    ")\n",
    "\n",
    "# text generation\n",
    "response = client.text_generation(\n",
    "    \"Tell me a story\",\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    details=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.generated_text)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use for Chat Format"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# chat completion\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.choices[0].message.content)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use OpenAI Client"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# initialize client pointing to vLLM endpoint\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"not-needed\",  # vLLM doesn't require an API key by default\n",
    ")\n",
    "\n",
    "# chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.choices[0].message.content)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text Generation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TGI"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setup"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "docker run --gpus all \\\n",
    "    --shm-size 1g \\\n",
    "    -p 8080:80 \\\n",
    "    -v ~/.cache/huggingface:/data \\\n",
    "    ghcr.io/huggingface/text-generation-inference:latest \\\n",
    "    --model-id HuggingFaceTB/SmolLM2-360M-Instruct \\\n",
    "    --max-total-tokens 4096 \\\n",
    "    --max-input-length 3072 \\\n",
    "    --max-batch-total-tokens 8192 \\\n",
    "    --waiting-served-ratio 1.2\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Use `InterenceClient`"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(model=\"http://localhost:8080\")\n",
    "\n",
    "# advanced parameters example\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    max_tokens=200,\n",
    "    top_p=0.95,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.choices[0].message.content)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# raw text generation\n",
    "response = client.text_generation(\n",
    "    \"Write a creative story about space exploration\",\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    "    do_sample=True,\n",
    "    details=True,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.generated_text)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Use OpenAI Client"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",\n",
    "    api_key=\"not-needed\"\n",
    ")\n",
    "\n",
    "# advanced parameters example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.8,  # higher for more creativity\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.choices[0].message.content)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Llama.cpp"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setup"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "llama-server \\\n",
    "    -hf HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8080 \\\n",
    "    -c 4096 \\\n",
    "    --threads 8 \\\n",
    "    --batch-size 512 \\\n",
    "    --n-gpu-layers 0\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Use `InterenceClient`"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T00:30:37.909920Z",
     "start_time": "2025-11-18T00:30:33.794443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    model=\"http://localhost:8080/v1\",\n",
    "    token=\"sk-no-key-required\"\n",
    ")\n",
    "\n",
    "# advanced parameters example\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    max_tokens=200,\n",
    "    top_p=0.95,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T00:30:39.700873Z",
     "start_time": "2025-11-18T00:30:39.698479Z"
    }
   },
   "cell_type": "code",
   "source": "print(response.choices[0].message.content)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the heart of a bustling city, there lived a young girl named Luna. She was a curious and adventurous soul with a vivid imagination. Luna was always fascinated by the world of dreams and the mysteries they held. She believed that dreams were a way of communicating with the divine, the cosmic, and the divine.\n",
      "\n",
      "One day, as Luna was wandering through her neighborhood, she stumbled upon a mysterious, ancient temple hidden in the heart of the city. The temple was old and had a beautiful, intricate, golden statue of a dreamer standing in its entrance. It was here that Luna discovered she had the ability to enter the world of dreams.\n",
      "\n",
      "Luna soon found herself capable of entering her own dreams, as well as those of others. She could see, hear, and even influence the dreams of others. It was as if she was a dreamer herself, living in the world of the dream. She could enter the dreams of her friends and loved ones, and she could\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# direct text generation\n",
    "response = client.text_generation(\n",
    "    \"Write a creative story about space exploration\",\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    "    details=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.generated_text)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Use OpenAI Client"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T00:32:16.054652Z",
     "start_time": "2025-11-18T00:32:11.946006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"sk-no-key-required\")\n",
    "\n",
    "# advanced parameters example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"smollm2-1.7b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.8,  # higher for more creativity\n",
    "    top_p=0.95,  # nucleus sampling probability\n",
    "    frequency_penalty=0.5,  # reduce repetition of frequent tokens\n",
    "    presence_penalty=0.5,  # reduce repetition by penalizing tokens already present\n",
    "    max_tokens=200,  # Maximum generation length\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T00:32:17.508691Z",
     "start_time": "2025-11-18T00:32:17.506246Z"
    }
   },
   "cell_type": "code",
   "source": "print(response.choices[0].message.content)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in the quaint town of Serendipity, there lived an ordinary man named Jack Harris. Jack was no superhero but he had his own ways to save the day when needed. He was an ordinary man with extraordinary skills that he never knew existed until one fateful day. \n",
      "\n",
      "It all began on a typical Monday morning when Jack woke up to find a tiny note on his bedside table. It was written in elegant handwriting and it read, \"You have a chance to change the world\". Jack was confused as to who could have left this note and what it meant. He thought it was just a prank played by his eccentric neighbor, but as he looked out of the window, he saw a strange object hovering above the town. \n",
      "\n",
      "It was a spaceship! The aliens had indeed come for him. But instead of being scared or excited, Jack was both. He had always been fascinated with science fiction and space movies but never imagined that he would actually meet\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Use Llama.cpp Native Library"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use llama-cpp-python package for direct model access\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# load model\n",
    "llm = Llama(\n",
    "    model_path=\"smollm2-1.7b-instruct.Q4_K_M.gguf\",\n",
    "    n_ctx=4096,  # Context window size\n",
    "    n_threads=8,  # CPU threads\n",
    "    n_gpu_layers=0,  # GPU layers (0 = CPU only)\n",
    ")\n",
    "\n",
    "# format prompt according to the model's expected format\n",
    "prompt = \"\"\"<|im_start|>system\n",
    "You are a creative storyteller.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Write a creative story\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "# generate response with precise parameter control\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=200,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    stop=[\"<|im_end|>\"],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(output[\"choices\"][0][\"text\"])"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### vLLM"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Use `InterenceClient`"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(model=\"http://localhost:8000/v1\")\n",
    "\n",
    "# advanced parameters example\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    max_tokens=200,\n",
    "    top_p=0.95,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.choices[0].message.content)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# direct text generation\n",
    "response = client.text_generation(\n",
    "    \"Write a creative story about space exploration\",\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    details=True,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.generated_text)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Use OpenAI Client"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"not-needed\"\n",
    ")\n",
    "\n",
    "# advanced parameters example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=200,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response.choices[0].message.content)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Use vLLM Python Native Interface"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# initialize model with advanced parameters\n",
    "llm = LLM(\n",
    "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    gpu_memory_utilization=0.85,\n",
    "    max_num_batched_tokens=8192,\n",
    "    max_num_seqs=256,\n",
    "    block_size=16,\n",
    ")\n",
    "\n",
    "# configure sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,  # higher for more creativity\n",
    "    top_p=0.95,  # consider top 95% probability mass\n",
    "    max_tokens=100,  # maximum length\n",
    "    presence_penalty=1.1,  # reduce repetition\n",
    "    frequency_penalty=1.1,  # reduce repetition\n",
    "    stop=[\"\\n\\n\", \"###\"],  # stop sequences\n",
    ")\n",
    "\n",
    "# generate text\n",
    "prompt = \"Write a creative story\"\n",
    "outputs = llm.generate(prompt, sampling_params)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(outputs[0].outputs[0].text)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# chat-style interactions\n",
    "chat_prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "]\n",
    "formatted_prompt = llm.get_chat_template()(chat_prompt)  # uses model's chat template\n",
    "outputs = llm.generate(formatted_prompt, sampling_params)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(outputs[0].outputs[0].text)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Token Selection and Sampling"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The process of generating text involves selecting the next token at each step.\n",
    "\n",
    "This selection process can be controlled through various parameters:\n",
    "\n",
    "1. **Raw Logits:** The initial output probabilities for each token\n",
    "2. **Temperature:** Controls randomness in selection (higher = more creative)\n",
    "3. **Top-p (Nucleus) Sampling:** Filters to top tokens making up X% of probability mass\n",
    "4. **Top-k Filtering:** Limits selection to k most likely tokens"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TGI"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "client.generate(\n",
    "    \"Write a creative story\",\n",
    "    temperature=0.8,  # higher for more creativity\n",
    "    top_p=0.95,  # consider top 95% probability mass\n",
    "    top_k=50,  # consider top 50 tokens\n",
    "    max_new_tokens=100,  # maximum length\n",
    "    repetition_penalty=1.1,  # reduce repetition\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Llama.cpp"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "response = client.completions.create(\n",
    "    model=\"smollm2-1.7b-instruct\",  # model name (can be any string for llama.cpp server)\n",
    "    prompt=\"Write a creative story\",\n",
    "    temperature=0.8,  # higher for more creativity\n",
    "    top_p=0.95,  # consider top 95% probability mass\n",
    "    frequency_penalty=1.1,  # reduce repetition\n",
    "    presence_penalty=0.1,  # reduce repetition\n",
    "    max_tokens=100,  # maximum length\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output = llm(\n",
    "    \"Write a creative story\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    max_tokens=100,\n",
    "    repeat_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### vLLM"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "params = SamplingParams(\n",
    "    temperature=0.8,  # higher for more creativity\n",
    "    top_p=0.95,  # consider top 95% probability mass\n",
    "    top_k=50,  # consider top 50 tokens\n",
    "    max_tokens=100,  # maximum length\n",
    "    presence_penalty=0.1,  # reduce repetition\n",
    ")\n",
    "llm.generate(\"Write a creative story\", sampling_params=params)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Controlling Repetition"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Both frameworks provide ways to prevent repetitive text generation:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TGI"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "client.generate(\n",
    "    \"Write a varied text\",\n",
    "    repetition_penalty=1.1,  # penalize repeated tokens\n",
    "    no_repeat_ngram_size=3,  # prevent 3-gram repetition\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Llama.cpp"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "response = client.completions.create(\n",
    "    model=\"smollm2-1.7b-instruct\",\n",
    "    prompt=\"Write a varied text\",\n",
    "    frequency_penalty=1.1,  # penalize frequent tokens\n",
    "    presence_penalty=0.8,  # penalize tokens already present\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output = llm(\n",
    "    \"Write a varied text\",\n",
    "    repeat_penalty=1.1,  # penalize repeated tokens\n",
    "    frequency_penalty=0.5,  # additional frequency penalty\n",
    "    presence_penalty=0.5,  # additional presence penalty\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### vLLM"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "params = SamplingParams(\n",
    "    presence_penalty=0.1,  # penalize token presence\n",
    "    frequency_penalty=0.1,  # penalize token frequency\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Length Control and Stop Sequences"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One can control generation length and specify when to stop:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TGI"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "client.generate(\n",
    "    \"Generate a short paragraph\",\n",
    "    max_new_tokens=100,\n",
    "    min_new_tokens=10,\n",
    "    stop_sequences=[\"\\n\\n\", \"###\"],\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Llama.cpp"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "response = client.completions.create(\n",
    "    model=\"smollm2-1.7b-instruct\",\n",
    "    prompt=\"Generate a short paragraph\",\n",
    "    max_tokens=100,\n",
    "    stop=[\"\\n\\n\", \"###\"],\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "output = llm(\"Generate a short paragraph\", max_tokens=100, stop=[\"\\n\\n\", \"###\"])"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### vLLM"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "params = SamplingParams(\n",
    "    max_tokens=100,\n",
    "    min_tokens=10,\n",
    "    stop=[\"###\", \"\\n\\n\"],\n",
    "    ignore_eos=False,\n",
    "    skip_special_tokens=True,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Memory Management"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Both frameworks implement advanced memory management techniques for efficient inference."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TGI"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TGI uses Flash Attention 2 and continuous batching:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "# Docker deployment with memory optimization\n",
    "docker run --gpus all -p 8080:80 \\\n",
    "    --shm-size 1g \\\n",
    "    ghcr.io/huggingface/text-generation-inference:latest \\\n",
    "    --model-id HuggingFaceTB/SmolLM2-1.7B-Instruct \\\n",
    "    --max-batch-total-tokens 8192 \\\n",
    "    --max-input-length 4096\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Llama.cpp"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "llama.cpp uses quantization and optimized memory layout:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "llama-server \\\n",
    "    -hf HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8080 \\\n",
    "    -c 2048 \\\n",
    "    --threads 4 \\\n",
    "    --n-gpu-layers 32 \\\n",
    "    --mlock \\       # lock memory to prevent swapping\n",
    "    --cont-batch    # enable continuous batching\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For models too large for a GPU, one can use CPU offloading:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "lama-server \\\n",
    "    -hf HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF \\\n",
    "    --n-gpu-layers 20 \\     # keep first 20 layers on GPU\n",
    "    --threads 8             # use more CPU threads for CPU layers\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### vLLM"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "vLLM uses PagedAttention for optimal memory management:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
    "    gpu_memory_utilization=0.85,\n",
    "    max_num_batched_tokens=8192,\n",
    "    block_size=16,\n",
    ")\n",
    "\n",
    "llm = LLM(engine_args=engine_args)"
   ]
  }
 ]
}
