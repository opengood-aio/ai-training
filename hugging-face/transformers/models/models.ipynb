{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MRC0e0KhQ0S"
   },
   "source": "# Models"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Outlines creating and using models using the `AutoModel` class from the Hugging Face `Transformers` library\n",
    "* Shows what happens when one instantiates an `AutoModel` object\n",
    "* All classes and functions are imported from the `Transformers` library"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:53.171683Z",
     "start_time": "2025-10-28T00:01:53.166926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_provider = \"bert\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = f\"{model_provider}/{model_name}\""
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating a Transformer"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Similar to instantiating a tokenizer, use the `from_pretrained` method on the `AutoModel` class to:\n",
    "    * Download and cache the model data\n",
    "* The checkpoint name corresponds to a specific model architecture and weights:\n",
    "    * A BERT model is used with a basic architecture:\n",
    "        * 12 layers\n",
    "        * 768 hidden size\n",
    "        * 12 attention heads\n",
    "        * Cased inputs\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "* The `AutoModel` class is simply a wrapper designed to fetch an appropriate model architecture given a checkpoint\n",
    "* It is considered *auto* because it will guess the appropriate model architecture and instantiate the correct model class\n",
    "* If the model architecture is known, a model can be instantiated using its direct class that defines its architecture:\n",
    "    * For example: `BertModel` will directly instantiate a BERT model"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:53.317218Z",
     "start_time": "2025-10-28T00:01:53.178733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading and Saving"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Saving a model is similar to saving a tokenizer\n",
    "* The `save_pretrained` method save's a model's weights and architecture configuration to disk:\n",
    "    * `config.json`: model architecture configuration\n",
    "    * `model.safetensors`: model weights\n",
    "* The `config.json` file contains all the necessary attributes to build the model architecture including:\n",
    "    * Checkpoint origination\n",
    "    * Transformers version used since last checkpoint save\n",
    "* The `model.safetensors` file is known as the state dictionary containing a model's weights\n",
    "* Both files work together:\n",
    "    * Configuration file is needed to know the model architecture\n",
    "    * Model weights are the parameters of the model\n",
    "* Use the `from_pretrained` method again to reuse a saved model"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:53.539565Z",
     "start_time": "2025-10-28T00:01:53.321960Z"
    }
   },
   "cell_type": "code",
   "source": "model.save_pretrained(\"~/\")",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:53.582535Z",
     "start_time": "2025-10-28T00:01:53.544038Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModel.from_pretrained(\"~/\")",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Publishing Models"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The Transformers library can publish models to Model Hub using one's Hugging Face account\n",
    "* The `push_to_hub` method is used to publish a model to the Model Hub\n",
    "* The `push_to_hub` method takes two arguments:\n",
    "    * `repo_id`: the name of the repository to publish the model to\n",
    "    * `token`: the token used to authenticate with the Model Hub"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Login to Hugging Face"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:53.593530Z",
     "start_time": "2025-10-28T00:01:53.587613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "582a7468e01a4b45babd7ca35def05e8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Push Model"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The model is pushed to Model Hub into a repository under one's namespace\n",
    "* Anyone can then load the model using the `from_pretrained` method"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:53.616846Z",
     "start_time": "2025-10-28T00:01:53.615227Z"
    }
   },
   "cell_type": "code",
   "source": "# model.push_to_hub(\"my-model\")",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:53.629824Z",
     "start_time": "2025-10-28T00:01:53.627925Z"
    }
   },
   "cell_type": "code",
   "source": "# model = AutoModel.from_pretrained(\"username/my-model\")",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Encoding Text"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Transformer models handle text by turning inputs into numbers\n",
    "* Text is split into tokens then transformed into numbers\n",
    "* The `AutoTokenizer` class is used to encode text into numbers\n",
    "* It returns a dictionary with the following fields:\n",
    "    * `input_ids`: the numerical representations of tokens\n",
    "    * `token_type_ids`: informs a model which part of the input is a given sentence\n",
    "    * `attention_mask`: indicates which tokens should be attended to or not"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:53.935593Z",
     "start_time": "2025-10-28T00:01:53.649913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:53.942740Z",
     "start_time": "2025-10-28T00:01:53.940818Z"
    }
   },
   "cell_type": "code",
   "source": "print(encoded_input)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Input IDs can be decoded back into their original text\n",
    "* The tokenizer added special tokens:\n",
    "    * \\[CLS\\]: classifies the input\n",
    "    * \\[SEP\\]: separates sentences\n",
    "* The model requires these special tokens\n",
    "* Not all models need special tokens\n",
    "* Only those pretrained with special tokens require them"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:53.961068Z",
     "start_time": "2025-10-28T00:01:53.958265Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(encoded_input[\"input_ids\"])",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Hello, I ' m a single sentence! [SEP]\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Multiple sentences can be encoded together\n",
    "    * Batching together\n",
    "    * Via a list\n",
    "* When passing multiple sentences, the tokenizer returns a list for each sentence for each dictionary value\n",
    "* One can also ask the tokenizer to return the tensors"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:53.970848Z",
     "start_time": "2025-10-28T00:01:53.968993Z"
    }
   },
   "cell_type": "code",
   "source": "encoded_input = tokenizer(\"How are you?\", \"I'm fine, thank you!\")",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.002227Z",
     "start_time": "2025-10-28T00:01:54.000292Z"
    }
   },
   "cell_type": "code",
   "source": "print(encoded_input)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1731, 1132, 1128, 136, 102, 146, 112, 182, 2503, 117, 6243, 1128, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.014177Z",
     "start_time": "2025-10-28T00:01:54.012154Z"
    }
   },
   "cell_type": "code",
   "source": "encoded_input = tokenizer(\"How are you?\", \"I'm fine, thank you!\", return_tensors=\"pt\")",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.029300Z",
     "start_time": "2025-10-28T00:01:54.026684Z"
    }
   },
   "cell_type": "code",
   "source": "print(encoded_input)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1731, 1132, 1128,  136,  102,  146,  112,  182, 2503,  117, 6243,\n",
      "         1128,  106,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The lists do not have the same length\n",
    "* Arrays and tensors need to be rectangular\n",
    "* One cannot convert lists to tensors"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Padding Inputs"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* By setting the `padding` parameter, the tokenizer will make all sentences the same length by adding a special padding token to the sentences that are shorter than the longest one\n",
    "* The result will be rectangular tensors\n",
    "* Padding tokens are encoded into input IDs with ID $0$\n",
    "* They have an attention mask value of $0$\n",
    "* The model will not analyze these tokens"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.040244Z",
     "start_time": "2025-10-28T00:01:54.037974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_input = tokenizer(\n",
    "    [\"How are you?\", \"I'm fine, thank you!\"], padding=True, return_tensors=\"pt\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.055220Z",
     "start_time": "2025-10-28T00:01:54.052383Z"
    }
   },
   "cell_type": "code",
   "source": "print(encoded_input)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1731, 1132, 1128,  136,  102,    0,    0,    0,    0],\n",
      "        [ 101,  146,  112,  182, 2503,  117, 6243, 1128,  106,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Truncating Inputs"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Tensors might get too big to be processed by a model\n",
    "* For instance, BERT is only pretrained with sequences up to 512 tokens\n",
    "* For sequences longer than a model can handle, they need to be truncated\n",
    "* Use the `truncation` parameter to perform sequence truncation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.069609Z",
     "start_time": "2025-10-28T00:01:54.067373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_input = tokenizer(\n",
    "    \"This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.\",\n",
    "    truncation=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.079927Z",
     "start_time": "2025-10-28T00:01:54.078265Z"
    }
   },
   "cell_type": "code",
   "source": "print(encoded_input[\"input_ids\"])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1188, 1110, 170, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1263, 5650, 119, 102]\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* Combining padding and truncation parameters, one can ensure the tensors are the size one needs"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.094649Z",
     "start_time": "2025-10-28T00:01:54.092288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_input = tokenizer(\n",
    "    [\"How are you?\", \"I'm fine, thank you!\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=5,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.110843Z",
     "start_time": "2025-10-28T00:01:54.108042Z"
    }
   },
   "cell_type": "code",
   "source": "print(encoded_input)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1731, 1132, 1128,  102],\n",
      "        [ 101,  146,  112,  182,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add Special Tokens"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Special tokens are important to some models, such as BERT\n",
    "* These tokens are added to better represent sentence boundaries:\n",
    "    * \\[CLS\\]: classifies the input\n",
    "    * \\[SEP\\]: separates sentences\n",
    "    * \\[PAD\\]: padding token\n",
    "    * \\[UNK\\]: unknown token"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.125700Z",
     "start_time": "2025-10-28T00:01:54.123572Z"
    }
   },
   "cell_type": "code",
   "source": "encoded_input = tokenizer(\"How are you?\")",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.139214Z",
     "start_time": "2025-10-28T00:01:54.137195Z"
    }
   },
   "cell_type": "code",
   "source": "print(encoded_input[\"input_ids\"])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1731, 1132, 1128, 136, 102]\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.153423Z",
     "start_time": "2025-10-28T00:01:54.150839Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(encoded_input[\"input_ids\"])",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] How are you? [SEP]'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Example"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Given the sequences:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:01:54.162251Z",
     "start_time": "2025-10-28T00:01:54.160564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once tokenized:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:02:59.555607Z",
     "start_time": "2025-10-28T00:02:59.552983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_sequences = [\n",
    "    [\n",
    "        101,\n",
    "        1045,\n",
    "        1005,\n",
    "        2310,\n",
    "        2042,\n",
    "        3403,\n",
    "        2005,\n",
    "        1037,\n",
    "    ],\n",
    "    [101, 1045, 5223, 2023, 2061, 2172, 999, 102],\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* This is a list of encoded sequences (list of lists)\n",
    "* Tensors only accept rectangular shapes (matrices)\n",
    "* This 2D array is already a rectangular shape and be converted to a tensor"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:03:01.421166Z",
     "start_time": "2025-10-28T00:03:01.417706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Using the tensors is simple: call the model with inputs!\n",
    "* Only the input IDs are required for the model"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:04:26.922820Z",
     "start_time": "2025-10-28T00:04:26.842735Z"
    }
   },
   "cell_type": "code",
   "source": "output = model(model_inputs)",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T00:05:07.867843Z",
     "start_time": "2025-10-28T00:05:07.864763Z"
    }
   },
   "cell_type": "code",
   "source": "print(output.last_hidden_state.shape)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 768])\n"
     ]
    }
   ],
   "execution_count": 76
  }
 ]
}
