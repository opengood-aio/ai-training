{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MRC0e0KhQ0S"
   },
   "source": "# Behind the Pipeline"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Outlines what the `pipeline` function from the Hugging Face `Transformers` library does behind the scenes\n",
    "* Uses a sentiment analysis example\n",
    "* Uses the `distilbert/distilbert-base-uncased-finetuned-sst-2-english` model\n",
    "* All classes and functions are imported from the `Transformers` library"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:18:59.073600Z",
     "start_time": "2025-10-27T01:18:59.071041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_provider = \"distilbert\"\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = f\"{model_provider}/{model_name}\"\n",
    "task_name = \"sentiment-analysis\""
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentiment Analysis Pipeline"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:18:59.642157Z",
     "start_time": "2025-10-27T01:18:59.090780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=task_name, model=model)\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing with Tokenizer"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Raw text needs to be tokenized and converted into integers for input into the model\n",
    "* This preprocessing must be done the same way as when the model was pretrained\n",
    "* Use the `AutoTokenizer` class via its `from_pretraining` method with the model's checkpoint name to:\n",
    "    * Automatically fetch the data associated with the model's tokenizer\n",
    "    * Cache the data so it can be reused as needed\n",
    "* Once the `tokenizer` object is set, text sentences can be passed to it\n",
    "* The `raw_inputs` variable defines an array of the text sentences to pass to the tokenizer\n",
    "* Transformer models only accept *tensors* as input\n",
    "* The `return_tensors` argument of the tokenizer specifies the type of tensors to return\n",
    "    * `pt` returns PyTorch tensors\n",
    "* The `inputs` variable is the output of the tokenizer to be used as input to the model\n",
    "    * It contains a dictionary of 2 keys:\n",
    "        * `input_ids`: Contains 2 rows of integers (one for each sentence) that are unique identifiers of the tokens in each sentence\n",
    "        * `attention_masks`"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:18:59.648166Z",
     "start_time": "2025-10-27T01:18:59.646135Z"
    }
   },
   "cell_type": "code",
   "source": "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:18:59.843886Z",
     "start_time": "2025-10-27T01:18:59.655675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:18:59.849202Z",
     "start_time": "2025-10-27T01:18:59.847635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:18:59.854954Z",
     "start_time": "2025-10-27T01:18:59.852540Z"
    }
   },
   "cell_type": "code",
   "source": "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:18:59.864841Z",
     "start_time": "2025-10-27T01:18:59.862259Z"
    }
   },
   "cell_type": "code",
   "source": "print(inputs)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Going through Model"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The pretrained model can be downloaded the same way as the tokenizer\n",
    "* Use the `AutoModel` class via its `from_pretrained` method with the model's checkpoint name to:\n",
    "    * Use the cached checkpoint used with the tokenizer\n",
    "    * Instantiate a model\n",
    "* This architecture only contains the base Transformer module:\n",
    "    * Given inputs, it outputs *hidden states* or *features*\n",
    "    * For each model input, a high-dimensional vector will be retrieved representing the contextual understanding of that input by the Transformer model\n",
    "    * Hidden states are useful on their own, but they are usually inputs to another part of the model known as the *head*\n",
    "* The vector output by the Transformer module is usually large and has 3 dimensions:\n",
    "    * Batch size: Number of sequences processed at a time (2 from the example)\n",
    "    * Sequence length: Length of the numerical representation of the sequence (16 from the example)\n",
    "    * Hidden size: Vector dimension of model input\n",
    "* The vector is highly dimensional because of the last value:\n",
    "    * The hidden size can be very large (768 is common and larger models reach 3072+)\n",
    "* The `outputs` variable contains attributes that shows the 3 dimensions of the vector\n",
    "    * This is noted via `torch.Size` from the `shape` attribute of the `last_hidden_state` attribute"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:18:59.943092Z",
     "start_time": "2025-10-27T01:18:59.875643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:18:59.980127Z",
     "start_time": "2025-10-27T01:18:59.948840Z"
    }
   },
   "cell_type": "code",
   "source": "outputs = model(**inputs)",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:18:59.987797Z",
     "start_time": "2025-10-27T01:18:59.985895Z"
    }
   },
   "cell_type": "code",
   "source": "print(outputs.last_hidden_state.shape)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Heads"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension\n",
    "* They are usually composed of one or a few linear layers\n",
    "* The output of the Transformer model is sent directly to the model head to be processed\n",
    "* The model is represented by its embeddings layer and the subsequent layers\n",
    "* The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token\n",
    "* The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Transfer Network with Head](Transfer%20Network%20with%20Head.png)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* For the example, a model with a sequence classification head is used\n",
    "    * Used to classify sentences as positive or negative\n",
    "* In place of the `AutoModel` class, the `AutoModelForSequenceClassification` is used\n",
    "* The dimensionality will be much lower for the shape of the `outputs` variable\n",
    "    * The model takes as input the high-dimensional vectors\n",
    "    * The output vectors contain 2 values, one per label\n",
    "* Since there are two sentences and two labels, the result from the model is of shape 2 x 2"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:19:00.101733Z",
     "start_time": "2025-10-27T01:18:59.997677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:19:00.129488Z",
     "start_time": "2025-10-27T01:19:00.107496Z"
    }
   },
   "cell_type": "code",
   "source": "outputs = model(**inputs)",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:19:00.137528Z",
     "start_time": "2025-10-27T01:19:00.135842Z"
    }
   },
   "cell_type": "code",
   "source": "print(outputs.logits.shape)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Postprocessing Output"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The model output is not probabilities but logits, which are the raw, unnormalized scores outputted by the last layer of the model\n",
    "* To convert to probabilities, the logits need to go through a SoftMax layer\n",
    "    * All Transformer models output logits\n",
    "    * The loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:19:00.183967Z",
     "start_time": "2025-10-27T01:19:00.181417Z"
    }
   },
   "cell_type": "code",
   "source": "print(outputs.logits)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:19:00.203427Z",
     "start_time": "2025-10-27T01:19:00.199688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:19:00.213371Z",
     "start_time": "2025-10-27T01:19:00.210947Z"
    }
   },
   "cell_type": "code",
   "source": "print(predictions)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* With the logits converted to probabilities, the model predicted:\n",
    "    * [0.0402, 0.9598] for the first sentence\n",
    "    * [0.9995, 0.0005] for the second sentence\n",
    "* To get the labels corresponding to each position, use the `id2label` attribute of the `model` variable's `config` attribute"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T01:19:00.223042Z",
     "start_time": "2025-10-27T01:19:00.220972Z"
    }
   },
   "cell_type": "code",
   "source": "model.config.id2label",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "One can conclude that the model predicted the following:\n",
    "\n",
    "* First sentence: `NEGATIVE: 0.0402, POSITIVE: 0.9598`\n",
    "* Second sentence: `NEGATIVE: 0.9995, POSITIVE: 0.0005`"
   ]
  }
 ]
}
